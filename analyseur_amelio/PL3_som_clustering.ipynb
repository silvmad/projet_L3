{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d12d794-2c9b-452b-8812-b2ea459a4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertModel, CamembertForSequenceClassification, Trainer, TrainingArguments, AutoModel\n",
    "#import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from clustertools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516edbe8-550a-4a1c-b57e-9f99b59cfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a6dafc-3163-4e32-957b-e16a5810b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"africain\", \"antisemite\", \"arabe\", \"asie\", \"capacitiste\", \"chretiens\", \"gitan\", \"lgbt\", \"miso\", \"musulman\"]\n",
    "\n",
    "cluster_dir = \"clusters/{}/\"\n",
    "datasets_dir = \"datasets/{}.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37f65f-fb4b-472f-bd7c-49fad4b33e88",
   "metadata": {},
   "source": [
    "### Récupération des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb37308a-e55b-4e7e-9bf1-f6d6001630b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msg(raw_dataset_file):\n",
    "    with open(raw_dataset_file, \"r\") as file:\n",
    "        raw_dataset = file.read().splitlines()\n",
    "    return raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40de3a-8441-46a3-a6b2-cad41284896b",
   "metadata": {},
   "source": [
    "### Encodage du jeu de données\n",
    "\n",
    "On charge le modèle depuis le disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b87b0d-1311-43b4-9661-85703aa1ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models/tweetbert\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125ae92d-8a3c-4e52-9acd-cc5cc737dd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/tweetbert were not used when initializing CamembertModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertModel were not initialized from the model checkpoint at models/tweetbert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(model_dir, do_lowercase=False)\n",
    "camembert = CamembertModel.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6198b2-22c5-47c7-8d9e-b88ce826f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = camembert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d549abe6-a51a-4f4e-82a6-d321b106c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sample):\n",
    "    encoded = []\n",
    "    for n in range(0, len(sample), 50):\n",
    "        batch = tokenizer(sample[n:n+50], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        torch.cuda.empty_cache()\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outp = camembert(**batch)\n",
    "        encoded.append(outp['last_hidden_state'][:,0,:].to('cpu'))\n",
    "    return torch.cat(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9a954-66e2-4489-9db5-3d4b63300863",
   "metadata": {},
   "source": [
    "### Analyse des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f9c20ee-5564-4708-9f00-8382fd6fd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CamembertForSequenceClassification.from_pretrained(\"models/tweetbert_val\")\n",
    "#torch.cuda.empty_cache()\n",
    "r = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf9c806-6c32-47ed-b5ef-d65fc25ccd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample):\n",
    "    pred = []\n",
    "    for n in range(0, len(sample), 10):\n",
    "        phrases = sample[n:n+10]\n",
    "        enc = tokenizer(phrases, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        enc.to('cuda:0')\n",
    "        with torch.no_grad():\n",
    "            outp = classifier(**enc)\n",
    "        p = torch.nn.functional.softmax(outp.logits, dim=1)\n",
    "        labels = torch.argmax(p, dim=1)\n",
    "        pred.append(labels)\n",
    "    return torch.cat(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90274c98-aab5-4659-8095-80eafd12ca09",
   "metadata": {},
   "source": [
    "### Partitionnement de tous les datasets et sauvegarde des clusters \n",
    "On ne sauvegarde que les clusters dont le pourcentage de messages haineux est supérieur ou égal à 80% ou inférieur ou égal à 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7310d24f-3825-4df9-9b6d-aa16d4a8834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travail sur le dataset africain.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.936816503551681\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset antisemite.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.7117942081307858\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset arabe.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.8146910193865977\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset asie.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.9508106050659635\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset capacitiste.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.9656264492042212\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset chretiens.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.876493057547927\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset gitan.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.9519548521433399\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset lgbt.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.984495841416363\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset miso.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.8566456407456708\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n",
      "Travail sur le dataset musulman.\n",
      "Chargement des messages.\n",
      "Encodage.\n",
      "Partitionnement.\n",
      " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
      " quantization error: 1.6681466988563378\n",
      "Évaluation des résultats.\n",
      "Sauvegarde des résultats.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"Travail sur le dataset {}.\".format(dataset))\n",
    "    print(\"Chargement des messages.\")\n",
    "    raw_dataset = load_msg(datasets_dir.format(dataset))\n",
    "    print(\"Encodage.\")\n",
    "    dataset_encoded = encode(raw_dataset)\n",
    "    pca = PCA(n_components=2)\n",
    "    two_dim_dataset = pca.fit_transform(dataset_encoded)\n",
    "    cd = cluster_dir.format(dataset)\n",
    "    init_globals([msg.split() for msg in raw_dataset], raw_dataset, two_dim_dataset, cd)\n",
    "    m = 10\n",
    "    n = 10\n",
    "    print(\"Partitionnement.\")\n",
    "    som = MiniSom(m, n, 768, neighborhood_function='gaussian', random_seed=0)\n",
    "    som.pca_weights_init(dataset_encoded)\n",
    "    som.train(dataset_encoded, 100000, verbose=True)\n",
    "    pred = []\n",
    "    for msg in dataset_encoded:\n",
    "        winner = som.winner(msg)\n",
    "        pred.append(winner[0]*n+winner[1])\n",
    "    res, wlists, mlists, cmlists, e2dmlists, mfw, hkw, clust_n_msg = parse(pred)\n",
    "    print(\"Évaluation des résultats.\")\n",
    "    hate_pred = []\n",
    "    for mlist in mlists:\n",
    "        hate_pred.append(predict(mlist))\n",
    "    hper = []\n",
    "    for i, p in enumerate(hate_pred):\n",
    "        bc = torch.bincount(p, minlength=2)\n",
    "        perc = (bc[1] / (bc[0]+bc[1])).item() * 100\n",
    "        hper.append(perc)\n",
    "    if not os.path.exists(cd): \n",
    "        os.mkdir(cd)\n",
    "    print(\"Sauvegarde des résultats.\")\n",
    "    for i in range(100):\n",
    "        if hper[i] >= 80 or hper[i] <= 20:\n",
    "            lab = \"haineux\" if hper[i] >= 80 else \"normal\"\n",
    "            save_cluster_info(cd + \"info.txt\", clust_n_msg[i], mfw[i], hkw[i], hper[i], None)\n",
    "            save_cluster_raw_msg(mlists[i], cd + \"cluster{}_{}.txt\".format(i, lab))\n",
    "    print()\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ab575-920a-40e5-b607-218c2e89004c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
